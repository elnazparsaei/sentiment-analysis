!pip install nltk


import os
from nltk.corpus import stopwords
from nltk import word_tokenize
from string import punctuation
import random


os.listdir('data/neg')


s = str.maketrans('', '', punctuation)#tranlator
a = 'salam!@'
a.translate(s)


stop_words = stopwords.words('english')


negative_documents = []
max_len_negative = 0
for file in os.listdir('data/neg'):
    if not file.endswith('ipynb_checkpoints'):
        with open('data/neg/' + file) as f:
            text = f.read()
            tokens = word_tokenize(text)
            translator = str.maketrans('', '', punctuation)
            tokens = [w.translate(translator) for w in tokens]
            tokens = [w for w in tokens if not w in stop_words]
            if len(tokens) > max_len_negative:
                max_len_negative = len(tokens)
            negative_documents.append(' '.join(tokens))
len(negative_documents)


max_len_negative


positive_documents = []
max_len_positive = 0
for file in os.listdir('data/pos'):
    with open('data/pos/' + file) as f:
        text = f.read()
        tokens = word_tokenize(text)
        translator = str.maketrans('', '', punctuation)
        tokens = [w.translate(translator) for w in tokens]
        tokens = [w for w in tokens if not w in stop_words]
        if len(tokens) > max_len_positive:
            max_len_positive = len(tokens)
        positive_documents.append(' '.join(tokens))
len(positive_documents)


max_len_positive


max_len = max(max_len_negative, max_len_positive)


random.shuffle(negative_documents)
random.shuffle(positive_documents)


X_train = negative_documents[:800] + positive_documents[:800]


len(X_train)


y_train = [0 for _ in range(800)] + [1 for _ in range(800)]


len(y_train)


X_test = negative_documents[800:] + positive_documents[800:]
len(X_test)


y_test = [0 for _ in range(200)] + [1 for _ in range(200)]


from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import plot_model
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Flatten, Embedding, Conv1D, MaxPool1D, Dropout
from tensorflow.keras.layers import concatenate


max_len


tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)


import pickle
with open('tokenizer.h5', 'wb') as f:
    pickle.dump(tokenizer, f)


tokenizer


vocab_len = len(tokenizer.word_index) + 1


vocab_len


encoded = tokenizer.texts_to_sequences(X_train)


encoded[0]


padded = pad_sequences(encoded, maxlen=max_len, padding='post')


padded.shape


input1 = Input(shape=(max_len,))
embedding1 = Embedding(vocab_len, 100)(input1)
conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)
drop1 = Dropout(0.5)(conv1)
pool1 = MaxPool1D(pool_size=2)(drop1)
flat1 = Flatten()(pool1)
dense1 = Dense(10, activation='relu')(flat1)
output = Dense(1, activation='sigmoid')(dense1)


model = Model(inputs=[input1], outputs=output)


model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])


model.summary()


plot_model(model, show_shapes=True)


encoded_test = tokenizer.texts_to_sequences(X_test)
padded_test = pad_sequences(encoded_test, maxlen=max_len, padding='post')


padded_test.shape


import numpy as np


model.fit([padded], np.array(y_train), epochs=10, batch_size=20, validation_data=([padded_test], np.array(y_test)))


model.save('textcnn.h5')


from tensorflow.keras.models import load_model


model = load_model('textcnn.h5')


with open('tokenizer.h5', 'rb') as f:
    tokenizer = pickle.load(f)


tokenizer.word_index


text = """
moviemaking is a lot like being the general manager of an nfl team in the post-salary cap era -- you've got to know how to allocate your resources . 
every dollar spent on a free-agent defensive tackle is one less dollar than you can spend on linebackers or safeties or centers . 
in the nfl , this leads to teams like the detroit lions , who boast a superstar running back with a huge contract , but can only field five guys named herb to block for him . 
in the movies , you end up with films like " spawn " , with a huge special-effects budget but not enough money to hire any recognizable actors . 
jackie chan is the barry sanders of moviemaking . 
he spins and darts across the screen like sanders cutting back through the defensive line . 
watching jackie in operation condor as he drives his motorcycle through the crowded streets of madrid , fleeing an armada of pursuers in identical black compact cars , is reminiscent of sanders running for daylight with the chicago bears in hot pursuit , except that sanders doesn't have to worry about rescuing runaway baby carriages . 
but like the lions star , jackie doesn't have anybody to block for him . 
almost every cent that's invested in a jackie chan movie goes for stunts , and as chan does his own stunts , the rest of the money goes to pay his hospital bills . 
this leaves about 75 cents to pay for things like directors ( chan directs ) , scripts and dubbing and supporting characters , not to mention the hideous title sequence . 
this also explains why the movie was shot in odd places like morocco and spain . 
 ( chan's first release in this country , " rumble in the bronx " , was supposedly set in new york , but was filmed in vancouver , and in the chase scenes the canadian rockies are clearly visible . ) 
heck , jackie doesn't even have enough money for a haircut , looks like , much less a personal hairstylist . 
in condor , chan plays the same character he's always played , himself , a mixture of bruce lee and tim allen , a master of both kung-fu and slapstick-fu . 
jackie is sent by the un to retrieve a cache of lost nazi gold in the north african desert , and is chased by a horde of neo-nazi sympathizers and two stereotypical arabs ( one of the things i like about jackie chan movies : no political correctness ) . 
he is joined by three women , who have little to do except scream , " jackie , save us ! " , and misuse firearms . 
the villain is an old nazi whose legs were broken in the secret base so that he has to be carried everywhere , and he's more pathetic than evil . 
en route , we have an extended motorcycle chase scene , a hilarious fight in the moroccan version of motel 6 with the neo-nazis , and two confrontations with savage natives . 
once at the secret desert base , there is a long chop-socky sequence , followed by the film's centerpiece , a wind-tunnel fight that's even better than the one in face/off . 
this is where the money was spent , on well-choreographed kung-fu sequences , on giant kevlar hamster balls , on smashed-up crates of bananas , and on scorpions . 
ignore the gaping holes in the plot ( how , exactly , if the villain's legs were broken , did he escape from the secret nazi base , and why didn't he take the key with him ? ) . 
don't worry about the production values , or what , exactly , the japanese girl was doing hitchhiking across the sahara . 
just go see the movie . 
operation condor has pretentions of being a " raiders of the lost ark " knockoff , but one wonders what jackie could do with the raiders franchise blocking for him -- with a lawrence kazdan screenplay , a john williams score , spielberg directing and george lucas producing , condor might be an a+ movie . 
however , you've got to go with what you've got , and what you've got in jackie chan is something special -- a talent that mainstream hollywood should , could , and ought to utilize .
"""


tokens = word_tokenize(text)
translator = str.maketrans('', '', punctuation)
tokens = [w.translate(translator) for w in tokens]
tokens = [w for w in tokens if not w in stop_words]
text = ' '.join(tokens)


text


text = tokenizer.texts_to_sequences([text])[0]
text


text = pad_sequences([text], maxlen=max_len, padding='post')
text


model.predict(text)


pred = model.predict(text)
if float(pred) > 0.5:
    print('Positive')
else:
    print('Negative')












